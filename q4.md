## Question 4: 

Assume these datasets are used in part of a pipeline where the file that arrives may contain unwanted duplicates, incorrect datatypes, missing dates or other data quality aberrations. What things could be done programmatically to make sure the input data is of adequate quality and improve the pipeline?

___

The question asks about things we can do programmatically- and I'll get to that...

However, first of all I think it is worth mentioning that when we are dealing with data ingestion from upstream data sources, if the possibility exists that we can communicate with  upstream 'stakeholders' (in this example Stats NZ) that are responsible for generating this data, this is the ideal scenario as we can feed-back information about the data quality, aberrations etc.

In fact, during the course of completing this technical test I spoke with Reuben Harrison, the Insights Analyst at Stats NZ that is involved in the generation of the Business Financial Data to get a better understanding of the dataset. His name and phone number were listed on the Release web-page under 'Technical enquiries', and it made sense to me to 'go to the source', so that I could get as best an understanding of the dataset as possible.

Communication is a fundamental skill, and where possible we should leverage it to ensure we achieve the best possible outcome.

___



