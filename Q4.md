# Question 4: 

Assume these datasets are used in part of a pipeline where the file that arrives may contain unwanted duplicates, incorrect datatypes, missing dates or other data quality aberrations. What things could be done programmatically to make sure the input data is of adequate quality and improve the pipeline?

___

The question asks about things we can do programmatically- and I'll get to that...

However, first of all I think it is worth mentioning that when we are dealing with data ingestion from upstream data sources, if the possibility exists that we can communicate with  upstream 'stakeholders' (in this example Stats NZ) that are responsible for generating this data, this is the ideal scenario as we can feed-back information about the data quality, aberrations etc.

In fact, during the course of completing this technical test I spoke with Reuben Harrison, the Insights Analyst at Stats NZ involved in the generation of the Business Financial Data to get a better understanding of the dataset. His name and phone number were listed on the Release webpage (see EDA section) under 'Technical enquiries'. It made sense to me to 'go to the source', so that I could get as best an understanding of the dataset as possible prior to delving into the question set.

Communication is a fundamental skill, and where possible we should leverage it to ensure we achieve the best possible outcome.

___

In an ETL pipeline, it's not always the case that the data received from the source system is of 'good quality'. There are ways and means to handle messy data, but before we blindly follow them it is of the utmost importance ask ourselves the 'why' of the action we are taking, and "does our strategy make sense?"

For example, we might ask ourselves the following questions if there is data missing in in the dataset:

* _Are there specific reasons as to why data is missing, and does that hold importance for the analysis we are performing?_

* _What is the proportion of missing data?_

* _Does our strategy of handling the missing data, for example, is data imputation sensible and does it make sense for the specific dataset we are working with?_

* _What are the possible implications of us undertaking this action?_

Many machine learning algorithms are unable to natively support missing data, and we have to make a call if we are going to perform data imputation, or if we simply drop that feature or row from the dataset. By asking ourselves the questions above, we ensure that the strategy we undertake is sensible for the analysis we are performing, and we are able to communicate and understand the limitations of our analysis or machine learning models.

___

### Examples of programatically handling source data to ensure is of adequate quality:

In the code examples below, I use the `pandas` python library- a "fast, powerful, flexible and easy to use open source data analysis and manipulation tool,
built on top of the Python programming language." (https://pandas.pydata.org/)

**Removal of duplicates**
  
  ```python
  # Remove duplicates in column_name, keeping only first occurance
  df = df.drop_duplicates(subset='column_name', keep='first')
  ```

**Correction of Datatypes**

  ```python
  # Generic dtype
  df['column_name'] = df['column_name'].astype('desired_dtype')
  
  # Datatime column
  df['date_column'] = pd.to_datetime(df['date_column'])
  ```

**Missing Data- Imputation**

  ```python
  # Replace missing values with zero
  df = df.fillna(value=0)
  ```

**Missing Data- Remove missing values**

  ```python
  # Drop any row containing NaN
  df = df.dropna(axis='index', how='any')

  # Drop any column containing only NaN
  df = df.dropna(axis='columns', how='all')
  ```


**Date Conversion**

   ```python
   # Convert date_column to datetime dtype
  df['date_column'] = pd.to_datetime(df['date_column'])
  ```

**Automated Data Quality Checks**

  ```python
  # Validate that all values in column_name are not NaN
  assert df['column_name'].isna().all(), "NaN values found in column_name"
  ```
